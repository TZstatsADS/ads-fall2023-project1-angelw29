ylab("Word Frequency")+
coord_flip()
bag_of_words_female <-  hm_data_female %>%
unnest_tokens(word, text)
word_count_female <- bag_of_words_female %>%
count(word, sort = TRUE)
word_count_female[1:15,] %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
labs(title = "               Word Frequency in Happy Moments for mothers")+
xlab(NULL) +
ylab("Word Frequency")+
coord_flip()
word
bag_of_words_female <-  hm_data_female %>%
unnest_tokens(word, text)
word_count_female <- bag_of_words_female %>%
count(word, sort = TRUE)
word_count_female[1:15,] %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
labs(title = "               Word Frequency in Happy Moments for mothers")+
xlab(NULL) +
ylab("Word Frequency")+
coord_flip()
hm_data_female <- hm_data[hm_data$gender=="f",]
bag_of_words_female <-  hm_data_female %>%
unnest_tokens(word, text)
word_count_female <- bag_of_words_female %>%
count(word, sort = TRUE)
word_count_female[1:15,] %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
labs(title = "               Word Frequency in Happy Moments for mothers")+
xlab(NULL) +
ylab("Word Frequency")+
coord_flip()
hm_data_male <- hm_data[hm_data$gender=="m",]
bag_of_words_male <-  hm_data_male %>%
unnest_tokens(word, text)
word_count_male <- bag_of_words_male %>%
count(word, sort = TRUE)
hm_data_male <- hm_data[hm_data$gender=="m",]
bag_of_words_male <-  hm_data_male %>%
unnest_tokens(word, text)
word_count_male <- bag_of_words_male %>%
count(word, sort = TRUE)
word_count_male[1:15,] %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
labs(title = "               Word Frequency in Happy Moments for women")+
xlab(NULL) +
ylab("Word Frequency")+
coord_flip()
bag_of_words_female <-  hm_data_female %>%
unnest_tokens(word, text)
word_count_female <- bag_of_words_female %>%
count(word, sort = TRUE)
word_count_female[1:15,] %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
labs(title = "               Word Frequency in Happy Moments for women")+
xlab(NULL) +
ylab("Word Frequency")+
coord_flip()
hm_data_male <- hm_data[hm_data$gender=="m",]
bag_of_words_male <-  hm_data_male %>%
unnest_tokens(word, text)
word_count_male <- bag_of_words_male %>%
count(word, sort = TRUE)
word_count_male[1:15,] %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
labs(title = "               Word Frequency in Happy Moments for women")+
xlab(NULL) +
ylab("Word Frequency")+
coord_flip()
data("stop_words")
word <- c("happy","ago","yesterday","lot","today","months","month",
"happier","happiest","last","week","past","day")
stop_words <- stop_words %>%
bind_rows(mutate(tibble(word), lexicon = "updated"))
hm_data_male <- hm_data[hm_data$gender=="m",]
bag_of_words_male <-  hm_data_male %>%
unnest_tokens(word, text)
word_count_male <- bag_of_words_male %>%
count(word, sort = TRUE)
word_count_male[1:15,] %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
labs(title = "               Word Frequency in Happy Moments for women")+
xlab(NULL) +
ylab("Word Frequency")+
coord_flip()
data("stop_words")
word <- c("happy","ago","yesterday","lot","today","months","month",
"happier","happiest","last","week","past","day")
stop_words <- stop_words %>%
bind_rows(mutate(tibble(word), lexicon = "updated"))
hm_data_male <- hm_data[hm_data$gender=="m",]
bag_of_words_male <-  hm_data_male %>%
unnest_tokens(word, text)
word_count_male <- bag_of_words_male %>%
count(word, sort = TRUE)
word_count_male[1:15,] %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
labs(title = "               Word Frequency in Happy Moments for women")+
xlab(NULL) +
ylab("Word Frequency")+
coord_flip()
knitr::opts_chunk$set(echo = TRUE)
library(tm)
library(tidytext)
library(tidyverse)
library(DT)
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/cleaned_hm.csv'
hm_data <- read_csv(urlfile)
corpus <- VCorpus(VectorSource(hm_data$cleaned_hm))%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords, character(0))%>%
tm_map(stripWhitespace)
word <- c("happy","ago","yesterday","lot","today","months","month",
"happier","happiest","last","week","past","day", "time")
knitr::opts_chunk$set(echo = TRUE)
library(tm)
library(tidytext)
library(tidyverse)
library(DT)
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/cleaned_hm.csv'
hm_data <- read_csv(urlfile)
corpus <- VCorpus(VectorSource(hm_data$cleaned_hm))%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords, character(0))%>%
tm_map(stripWhitespace)
stemmed <- tm_map(corpus, stemDocument) %>%
tidy() %>%
select(text)
dict <- tidy(corpus) %>%
select(text) %>%
unnest_tokens(dictionary, text)
data("stop_words")
word <- c("happy","ago","yesterday","lot","today","months","month",
"happier","happiest","last","week","past","day", "time")
stop_words <- stop_words %>%
bind_rows(mutate(tibble(word), lexicon = "updated"))
completed <- stemmed %>%
mutate(id = row_number()) %>%
unnest_tokens(stems, text) %>%
bind_cols(dict) %>%
anti_join(stop_words, by = c("dictionary" = "word"))
completed <- completed %>%
group_by(stems) %>%
count(dictionary) %>%
mutate(word = dictionary[which.max(n)]) %>%
ungroup() %>%
select(stems, word) %>%
distinct() %>%
right_join(completed) %>%
select(-stems)
completed <- completed %>%
group_by(id) %>%
summarise(text = str_c(word, collapse = " ")) %>%
ungroup()
hm_data <- hm_data %>%
mutate(id = row_number()) %>%
inner_join(completed)
datatable(hm_data)
write_csv(hm_data, "../output/processed_moments.csv")
hm_data
hm_data <- hm_data %>%
inner_join(demo_data, by = "wid") %>%
select(wid,
original_hm,
gender,
marital,
parenthood,
reflection_period,
age,
country,
ground_truth_category,
text) %>%
mutate(count = sapply(hm_data$text, wordcount)) %>%
filter(gender %in% c("m", "f")) %>%
filter(marital %in% c("single", "married")) %>%
filter(parenthood %in% c("n", "y")) %>%
filter(reflection_period %in% c("24h", "3m")) %>%
mutate(reflection_period = fct_recode(reflection_period,
months_3 = "3m", hours_24 = "24h"))
hm_data_f_USA <- hm_data[hm_data$gender=="f" & hm_data$marital=="married",]
hm_data_f_notUSA <- hm_data[hm_data$gender=="f" & hm_data$marital!="single",]
hm_data_female <- hm_data[hm_data$gender=="f",]
bag_of_words_female <-  hm_data_female %>%
unnest_tokens(word, text)
word_count_female <- bag_of_words_female %>%
count(word, sort = TRUE)
word_count_female[1:15,] %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
labs(title = "               Word Frequency in Happy Moments for women")+
xlab(NULL) +
ylab("Word Frequency")+
coord_flip()
hm_data_male <- hm_data[hm_data$gender=="m",]
bag_of_words_male <-  hm_data_male %>%
unnest_tokens(word, text)
word_count_male <- bag_of_words_male %>%
count(word, sort = TRUE)
word_count_male[1:15,] %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
labs(title = "               Word Frequency in Happy Moments for women")+
xlab(NULL) +
ylab("Word Frequency")+
coord_flip()
hm_data_male <- hm_data[hm_data$gender=="m",]
bag_of_words_male <-  hm_data_male %>%
unnest_tokens(word, text)
word_count_male <- bag_of_words_male %>%
count(word, sort = TRUE)
word_count_male[1:15,] %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
labs(title = "               Word Frequency in Happy Moments for men")+
xlab(NULL) +
ylab("Word Frequency")+
coord_flip()
hm_data_USA <- hm_data[hm_data$country=="USA",]
bag_of_words_USA <-  hm_data_USA %>%
unnest_tokens(word, text)
word_count_USA <- bag_of_words_USA %>%
count(word, sort = TRUE)
word_count_male[1:15,] %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
labs(title = "               Word Frequency in Happy Moments for men")+
xlab(NULL) +
ylab("Word Frequency")+
coord_flip()
hm_data_USA <- hm_data[hm_data$country=="USA",]
bag_of_words_USA <-  hm_data_USA %>%
unnest_tokens(word, text)
word_count_USA <- bag_of_words_USA %>%
count(word, sort = TRUE)
word_count_USA[1:15,] %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
labs(title = "               Word Frequency in Happy Moments for men")+
xlab(NULL) +
ylab("Word Frequency")+
coord_flip()
hm_data_USA <- hm_data[hm_data$gender=="m" & hm_data$country=="USA",]
bag_of_words_USA <-  hm_data_USA %>%
unnest_tokens(word, text)
word_count_USA <- bag_of_words_USA %>%
count(word, sort = TRUE)
word_count_USA[1:15,] %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
labs(title = "               Word Frequency in Happy Moments for men")+
xlab(NULL) +
ylab("Word Frequency")+
coord_flip()
hm_data_USA <- hm_data[hm_data$gender=="m" & hm_data$country!="USA",]
bag_of_words_USA <-  hm_data_USA %>%
unnest_tokens(word, text)
word_count_USA <- bag_of_words_USA %>%
count(word, sort = TRUE)
word_count_USA[1:15,] %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
labs(title = "               Word Frequency in Happy Moments for men")+
xlab(NULL) +
ylab("Word Frequency")+
coord_flip()
hm_data_USA <- hm_data[hm_data$gender=="f" & hm_data$country!="USA",]
bag_of_words_USA <-  hm_data_USA %>%
unnest_tokens(word, text)
word_count_USA <- bag_of_words_USA %>%
count(word, sort = TRUE)
word_count_USA[1:15,] %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
labs(title = "               Word Frequency in Happy Moments for men")+
xlab(NULL) +
ylab("Word Frequency")+
coord_flip()
word <- c("happy","ago","yesterday","lot","today","months","month",
"happier","happiest","last","week","past","day", "time", "moment")
knitr::opts_chunk$set(echo = TRUE)
library(tm)
library(tidytext)
library(tidyverse)
library(DT)
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/cleaned_hm.csv'
hm_data <- read_csv(urlfile)
corpus <- VCorpus(VectorSource(hm_data$cleaned_hm))%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords, character(0))%>%
tm_map(stripWhitespace)
stemmed <- tm_map(corpus, stemDocument) %>%
tidy() %>%
select(text)
dict <- tidy(corpus) %>%
select(text) %>%
unnest_tokens(dictionary, text)
data("stop_words")
word <- c("happy","ago","yesterday","lot","today","months","month",
"happier","happiest","last","week","past","day", "time", "moment")
stop_words <- stop_words %>%
bind_rows(mutate(tibble(word), lexicon = "updated"))
completed <- stemmed %>%
mutate(id = row_number()) %>%
unnest_tokens(stems, text) %>%
bind_cols(dict) %>%
anti_join(stop_words, by = c("dictionary" = "word"))
completed <- completed %>%
group_by(stems) %>%
count(dictionary) %>%
mutate(word = dictionary[which.max(n)]) %>%
ungroup() %>%
select(stems, word) %>%
distinct() %>%
right_join(completed) %>%
select(-stems)
completed <- completed %>%
group_by(id) %>%
summarise(text = str_c(word, collapse = " ")) %>%
ungroup()
hm_data <- hm_data %>%
mutate(id = row_number()) %>%
inner_join(completed)
datatable(hm_data)
write_csv(hm_data, "../output/processed_moments.csv")
hm_data
hm_data_USA <- hm_data[hm_data$gender=="f" & hm_data$country!="USA",]
hm_data_mother <- hm_data[hm_data$gender=="f" & hm_data$parenthood=="y",]
urlfile<-'../data/cleaned_hm.csv'
hm_data <- read_csv(urlfile)
setwd("~/")
urlfile<-'../data/cleaned_hm.csv'
hm_data <- read_csv(urlfile)
setwd("~/Downloads")
knitr::opts_chunk$set(echo = TRUE)
packages.used=c("tm", "tidytext","tidyverse","DT","wordcloud","scales","gridExtra","ngram","igraph","ggraph","rsconnect")
# check packages that need to be installed.
packages.needed=setdiff(packages.used,
intersect(installed.packages()[,1],
packages.used))
# install additional packages
if(length(packages.needed)>0){
install.packages(packages.needed, dependencies = TRUE)
}
# load packages
library(tm)
library(tidytext)
library(tidyverse)
library(DT)
library(wordcloud)
library(scales)
library(gridExtra)
library(ngram)
library(igraph)
library(ggraph)
library(ggraph)
library(ggraph)
library(rsconnect)
print(R.version)
urlfile<-'../data/cleaned_hm.csv'
hm_data <- read_csv(urlfile)
setwd("/Users/angelwang/Desktop/fall 2023/4243/ads-fall2023-project1-angelw29")
urlfile<-'../data/cleaned_hm.csv'
hm_data <- read_csv(urlfile)
urlfile<-'../data/cleaned_hm.csv'
hm_data <- read_csv(urlfile)
setwd("/Users/angelwang/Desktop/fall 2023/4243/ads-fall2023-project1-angelw29")
urlfile<-'../data/cleaned_hm.csv'
urlfile<-'../data/cleaned_hm.csv'
hm_data <- read_csv(urlfile)
setwd("/Users/angelwang/Desktop/fall 2023/4243/ads-fall2023-project1-angelw29")
urlfile<-'../data/cleaned_hm.csv'
hm_data <- read_csv(urlfile)
setwd("/Users/angelwang/Desktop/fall 2023/4243/ads-fall2023-project1-angelw29")
getwd()
setwd("/Users/angelwang/Desktop/fall 2023/4243/ads-fall2023-project1-angelw29")
setwd("/Users/angelwang/Desktop/fall 2023/4243/ads-fall2023-project1-angelw29")
getwd()
setwd("/Users/angelwang/Desktop/fall 2023/4243/ads-fall2023-project1-angelw29")
setwd("/Users/angelwang/Desktop/fall 2023/4243/ads-fall2023-project1-angelw29")
setwd("/Users/angelwang/Desktop/fall 2023/4243/ads-fall2023-project1-angelw29")
setwd("/Users/angelwang/Desktop/fall 2023/4243/ads-fall2023-project1-angelw29")
getwd()
urlfile<-'../data/cleaned_hm.csv'
hm_data <- read_csv(urlfile)
getwd()
getwd()
urlfile<-'../data/cleaned_hm.csv'
urlfile<-'../data/cleaned_hm.csv'
hm_data <- read_csv(urlfile)
getwd()
urlfile<-'../data/cleaned_hm.csv'
hm_data <- read_csv(urlfile)
urlfile<-'../data/cleaned_hm.csv'
hm_data <- read_csv(urlfile)
urlfile<-'../data/cleaned_hm.csv'
hm_data <- read_csv(urlfile)
hm_data <- read_csv("../output/processed_moments.csv")
hm_data <- read_csv("../output/processed_moments.csv")
hm_data <- read_csv("../output/processed_moments.csv")
hm_data <- read_csv("../output/processed_moments.csv")
hm_data <- read_csv("../output/processed_moments.csv")
urlfile<-'../data/demographic.csv'
demo_data <- read_csv(urlfile)
urlfile<-'../demographic.csv'
demo_data <- read_csv(urlfile)
urlfile<-'../data/demographic.csv'
demo_data <- read_csv(urlfile)
urlfile<-'../data/clean_hm.csv'
demo_data <- read_csv(urlfile)
urlfile<-'../data/cleaned_hm.csv'
demo_data <- read_csv(urlfile)
urlfile
urlfile<-'../data/cleaned_hm.csv'
demo_data <- read_csv(urlfile)
urlfile<-'../data/cleaned_hm.csv'
hm_data <- read_csv(urlfile)
urlfile<-'../data/cleaned_hm.csv'
hm_data <- read_csv(urlfile)
urlfile<-'../data/cleaned_hm.csv'
hm_data <- read_csv(urlfile)
urlfile<-'/Users/angelwang/Desktop/fall 2023/4243/ads-fall2023-project1-angelw29/data/cleaned_hm.csv'
urlfile<-'/Users/angelwang/Desktop/fall 2023/4243/ads-fall2023-project1-angelw29/data/cleaned_hm.csv'
hm_data <- read_csv(urlfile)
getwd()
urlfile<-'../data/cleaned_hm.csv'
hm_data <- read_csv(urlfile)
getwd()
urlfile<-'../data/cleaned_hm.csv'
hm_data <- read_csv(urlfile)
urlfile<-'../data/demographic.csv'
demo_data <- read_csv(urlfile)
urlfile<-'../data/cleaned_hm.csv'
hm_data <- read_csv(urlfile)
write_csv(hm_data, "../output/processed_moments.csv")
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/cleaned_hm.csv'
hm_data <- read_csv(urlfile)
corpus <- VCorpus(VectorSource(hm_data$cleaned_hm))%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords, character(0))%>%
tm_map(stripWhitespace)
write_csv
write_csv(hm_data, "../output/processed_moments.csv")
write_csv(hm_data, "../output/processed_moments.csv")
write_csv(hm_data, "../output/processed_moments.csv")
write_csv(hm_data, "../output/processed_moments.csv")
write_csv(hm_data, "../output/processed_moments.csv")
hm_data
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/cleaned_hm.csv'
hm_data <- read_csv(urlfile)
hm_data <- hm_data %>%
inner_join(demo_data, by = "wid") %>%
select(wid,
original_hm,
gender,
marital,
parenthood,
reflection_period,
age,
country,
ground_truth_category,
text) %>%
mutate(count = sapply(hm_data$text, wordcount)) %>%
filter(gender %in% c("m", "f")) %>%
filter(marital %in% c("single", "married")) %>%
filter(parenthood %in% c("n", "y")) %>%
filter(reflection_period %in% c("24h", "3m")) %>%
mutate(reflection_period = fct_recode(reflection_period,
months_3 = "3m", hours_24 = "24h"))
knitr::opts_chunk$set(echo = TRUE)
packages.used=c("tm", "tidytext","tidyverse","DT","wordcloud","scales","gridExtra","ngram","igraph","ggraph","rsconnect")
# check packages that need to be installed.
packages.needed=setdiff(packages.used,
intersect(installed.packages()[,1],
packages.used))
# install additional packages
if(length(packages.needed)>0){
install.packages(packages.needed, dependencies = TRUE)
}
